---
title: "papers"
---

These are my notes from research papers I read. Each page's title is also a link to the abstract or PDF.

to read: [Google AI: optimizing multiple loss functions](https://ai.googleblog.com/2020/04/optimizing-multiple-loss-functions-with.html) [Google AI: reducing gender bias in Google Translate](https://ai.googleblog.com/2020/04/a-scalable-approach-to-reducing-gender.html) [Zoom In: An Introduction to Circuits](https://distill.pub/2020/circuits/zoom-in/) [Google AI: Neural Tangents](https://ai.googleblog.com/2020/03/fast-and-easy-infinitely-wide-networks.html) [Google AI: TensorFlow Quantum](https://ai.googleblog.com/2020/03/announcing-tensorflow-quantum-open.html) [SLIDE (fast CPU training)](https://arxiv.org/abs/1903.03129) [Google AI: Reformer](https://ai.googleblog.com/2020/01/reformer-efficient-transformer.html) [lottery ticket initialization](https://arxiv.org/abs/1906.02773) [Google AI: out-of-distribution detection](https://ai.googleblog.com/2019/12/improving-out-of-distribution-detection.html) [Large-Scale Multilingual Speech Recognition with E2E model](https://arxiv.org/abs/1909.05330) [E2E ASR from raw waveform](https://arxiv.org/abs/1806.07098) [Machine Theory of Mind](https://arxiv.org/abs/1802.07740v2) [Normalizing Flows](https://akosiorek.github.io/ml/2018/04/03/norm_flows.html) [Glow networks](https://arxiv.org/abs/1807.03039) [A Theory of Local Learning, the Learning Channel, and the Optimality of Backpropagation](https://arxiv.org/abs/1506.06472) [Why and When Deep Networks Avoid the Curse of Dimensionality](https://arxiv.org/abs/1611.00740) [Diversity is All You Need (Learning Skills without a Reward Function)](https://arxiv.org/abs/1802.06070v6) [World Models](https://arxiv.org/abs/1803.10122v4) [Relational inductive biases, deep learning, and graph networks](https://arxiv.org/abs/1806.01261) [Loss Surfaces of Multilayer Networks](https://arxiv.org/abs/1412.0233) [Visualizing the Loss Landscape of Neural Nets](https://arxiv.org/abs/1712.09913v3) [The Matrix Calculus You Need for Deep Learning](https://arxiv.org/abs/1802.01528v3) [Group Normalization](https://arxiv.org/abs/1803.08494v3) [Layer Normalization](https://arxiv.org/abs/1607.06450) [Artificial Intelligence Meets Natural Stupidity](http://www.cs.yorku.ca/~jarek/courses/ai/F11/naturalstupidity.pdf) [Qualitatively characterizing neural network optimization problems](https://arxiv.org/abs/1412.6544) [Strong Inference](http://knowledgecontext.org/COSMOS/Strong_Inference_(Platt).pdf) [A learning algorithm for continually running fully recurrent neural networks](http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.52.9724) [Adaptive multi-level hyper-gradient descent](https://arxiv.org/abs/2008.07277) [Rotate your networks: better weight consolidation and less catastrophic forgetting](https://arxiv.org/abs/1802.02950) [Attention is not *all* you need](http://proceedings.mlr.press/v139/dong21a/dong21a.pdf) [When BERT plays the lottery, all tickets are winning](https://arxiv.org/abs/2005.00561) [Right for the wrong reasons: diagnosing syntactic heuristics in NLI](https://aclanthology.org/P19-1334.pdf) [Switch transformers: scaling to trillion parameter models with simple and efficient sparsity](https://arxiv.org/abs/2101.03961) [Concrete problems in AI safety](https://arxiv.org/abs/1606.06565) [The power of scale for parameter-efficient prompt tuning](https://aclanthology.org/2021.emnlp-main.243/)
