<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>ai on Kyle Roth</title><link>https://kylrth.com/tags/ai/</link><description>Recent content in ai on Kyle Roth</description><generator>Hugo -- gohugo.io</generator><language>en-us</language><lastBuildDate>Tue, 24 Oct 2023 12:53:31 -0400</lastBuildDate><atom:link href="https://kylrth.com/tags/ai/index.xml" rel="self" type="application/rss+xml"/><item><title>Better, Nicer, Clearer, Fairer: a critical assessment of the movement for ethical artificial intelligence and machine learning</title><link>https://kylrth.com/paper/better-nicer-cleaner-fairer/</link><pubDate>Tue, 24 Oct 2023 12:53:31 -0400</pubDate><guid>https://kylrth.com/paper/better-nicer-cleaner-fairer/</guid><description>I will present this paper in the FATE (fairness, accountability, transparency, ethics) reading group tomorrow (2023-10-25). You can view the slides I&amp;rsquo;ll use here.
There are unresolved tensions in the algorithmic ethics world. Here are two examples:
Is inclusion always good? Gebru: &amp;ldquo;you can&amp;rsquo;t have ethical A.I. that&amp;rsquo;s not inclusive&amp;hellip; [a]nd whoever is creating the technology is setting the standards&amp;rdquo; Nelson: &amp;ldquo;&amp;hellip; I struggle to understand why we want to make black communities more cognizant in facial recognition systems that are disproportionately used for surveillance.</description></item></channel></rss>