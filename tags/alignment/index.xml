<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>alignment on Kyle Roth</title><link>https://kylrth.com/tags/alignment/</link><description>Recent content in alignment on Kyle Roth</description><generator>Hugo -- gohugo.io</generator><language>en-us</language><lastBuildDate>Fri, 10 Feb 2023 08:09:15 -0500</lastBuildDate><atom:link href="https://kylrth.com/tags/alignment/index.xml" rel="self" type="application/rss+xml"/><item><title>Artificial intelligence, values, and alignment</title><link>https://kylrth.com/paper/ai-values-alignment/</link><pubDate>Fri, 10 Feb 2023 08:09:15 -0500</pubDate><guid>https://kylrth.com/paper/ai-values-alignment/</guid><description>Behind each vision for ethically-aligned AI sits a deeper question. How are we to decide which principles or objectives to encode in AI—and who has the right to make these decisions—given that we live in a pluralistic world that is full of competing conceptions of value? Is there a way to think about AI value alignment that avoids a situation in which some people simply impose their views on others?</description></item><item><title>Unsolved problems in ML safety</title><link>https://kylrth.com/paper/unsolved-problems-ml-safety/</link><pubDate>Mon, 06 Feb 2023 11:39:33 -0500</pubDate><guid>https://kylrth.com/paper/unsolved-problems-ml-safety/</guid><description>This was a paper we presented about in Irina Rish&amp;rsquo;s neural scaling laws course (IFT6760A) in winter 2023. You can view the slides we used here, and the recording here (or my backup here).</description></item></channel></rss>