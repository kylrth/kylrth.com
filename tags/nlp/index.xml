<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>nlp on Kyle Roth</title><link>https://kylrth.com/tags/nlp/</link><description>Recent content in nlp on Kyle Roth</description><generator>Hugo -- gohugo.io</generator><language>en-us</language><lastBuildDate>Mon, 24 Jul 2023 11:35:13 -0400</lastBuildDate><atom:link href="https://kylrth.com/tags/nlp/index.xml" rel="self" type="application/rss+xml"/><item><title>"Low-resource" text classification: a parameter-free classification method with compressors</title><link>https://kylrth.com/paper/gzip-text-classification/</link><pubDate>Mon, 24 Jul 2023 11:35:13 -0400</pubDate><guid>https://kylrth.com/paper/gzip-text-classification/</guid><description>I presented this paper in Bang Liu&amp;rsquo;s research group meeting on 2023-07-24. You can view the slides I used here.
It seems like the authors made a mistake that inflated the scores for the multilingual experiments, according to Ken Schutte.</description></item><item><title>Whisper: robust speech recognition via large-scale weak supervision</title><link>https://kylrth.com/paper/whisper/</link><pubDate>Fri, 30 Sep 2022 09:46:29 -0400</pubDate><guid>https://kylrth.com/paper/whisper/</guid><description>This was a paper I presented about in Bang Liu&amp;rsquo;s research group meeting on 2022-09-30. You can view the slides I used here.</description></item><item><title>Selective annotation makes language models better few-shot learners</title><link>https://kylrth.com/paper/selective-annotation/</link><pubDate>Tue, 13 Sep 2022 11:26:21 -0400</pubDate><guid>https://kylrth.com/paper/selective-annotation/</guid><description>Selective annotation chooses a pool of samples to annotate from a large set of unlabeled data. The main result of the paper is that when this is combined with item-specific prompt retrieval the performance drastically improves (&amp;gt;10% relative gain and lower performance variance). Interestingly, selective annotation does not help for finetuning, or when the prompts are randomly selected. They call their selective annotation method &amp;ldquo;vote-\(k\)&amp;rdquo;.
selective annotation method permalink Vote-\(k\) essentially creates a network of similaraccording to Sentence-BERT unlabeled instances, and then selects from them with a network importance score that is discounted to promote diversityThe discounting is performed by iteratively adding to the selection set, each time penalizing new nodes for being close to nodes that are already in the selection set.</description></item><item><title>Continual-T0: progressively instructing 50+ tasks to language models without forgetting</title><link>https://kylrth.com/paper/continual-t0/</link><pubDate>Thu, 02 Jun 2022 15:28:55 -0400</pubDate><guid>https://kylrth.com/paper/continual-t0/</guid><description>This was a paper I presented about in Bang Liu&amp;rsquo;s research group meeting on 2022-06-06. You can view the slides I used here.
Continual-T0 (CT0) extends T0 by progressively training it on 8 unseen language generation tasks, while retaining a replay buffer of 1% of the original training data to preserve performance. The result is a model that maintains nearly all of its performance on previous tasks while learning the new tasks.</description></item><item><title>Multitask prompted training enables zero-shot task generalization (T0)</title><link>https://kylrth.com/paper/t0/</link><pubDate>Fri, 27 May 2022 17:05:02 -0400</pubDate><guid>https://kylrth.com/paper/t0/</guid><description>T0 builds on T5 by fine-tuning on more natural prompts and testing the model&amp;rsquo;s generalization to held-out tasks.
Compare the training format diagrams for T5 (top) and T0 (bottom):
Intuitively, the T0 prompts are more likely to be similar to implicit/explicit prompting that&amp;rsquo;s present in the pretraining data. The authors created several prompts for each dataset.
results permalink Our experiments study two questions. First, does multitask prompted training improve generalization to held-out tasks?</description></item><item><title>WordBurner beta</title><link>https://kylrth.com/post/wordburner/</link><pubDate>Mon, 18 Apr 2022 23:08:52 -0400</pubDate><guid>https://kylrth.com/post/wordburner/</guid><description>Update 2022-04-27: The beta is over, but the apk is still installable with the instructions below and any feedback sent from inside the app will be received by me. I&amp;rsquo;m going to be working on this more over the summer, and eventually publishing it on the app store. :)
Ever since learning Spanish, it has been a dream of mine to create a vocabulary study app that meets my needs. Duolingo won&amp;rsquo;t cover advanced vocabulary, Anki requires manually-generated decks, and other apps have expensive subscription plans.</description></item><item><title>PaLM</title><link>https://kylrth.com/paper/palm/</link><pubDate>Mon, 11 Apr 2022 12:17:25 -0400</pubDate><guid>https://kylrth.com/paper/palm/</guid><description>This was a paper I presented about in Bang Liu&amp;rsquo;s research group meeting on 2022-04-11. You can view the slides I used here.</description></item><item><title>It's not just size that matters: small language models are also few-shot learners</title><link>https://kylrth.com/paper/not-just-size-that-matters/</link><pubDate>Fri, 18 Feb 2022 13:13:54 -0500</pubDate><guid>https://kylrth.com/paper/not-just-size-that-matters/</guid><description>We presented this paper as a mini-lecture in Bang Liu&amp;rsquo;s IFT6289 course in winter 2022. You can view the slides we used here.</description></item><item><title>A sensitivity analysis of (and practitionersâ€™ guide to) convolutional neural networks for sentence classification</title><link>https://kylrth.com/paper/cnn-sentence/</link><pubDate>Wed, 02 Feb 2022 15:35:00 -0500</pubDate><guid>https://kylrth.com/paper/cnn-sentence/</guid><description>This post was created as an assignment in Bang Liu&amp;rsquo;s IFT6289 course in winter 2022. The structure of the post follows the structure of the assignment: summarization followed by my own comments.
paper summarization permalink Word embeddings have gotten so good that state-of-the-art sentence classification can often be achieved with just a one-layer convolutional network on top of those embeddings. This paper dials in on the specifics of training that convolutional layer for this downstream sentence classification task.</description></item><item><title>Learning transferable visual models from natural language supervision (CLIP)</title><link>https://kylrth.com/paper/clip/</link><pubDate>Wed, 02 Feb 2022 12:35:03 -0500</pubDate><guid>https://kylrth.com/paper/clip/</guid><description>This post was created as an assignment in Irina Rish&amp;rsquo;s neural scaling laws course (IFT6167) in winter 2022. The post contains no summarization, only questions and thoughts.
This concept of wide vs. narrow supervision (rather than binary &amp;ldquo;supervised&amp;rdquo; and &amp;ldquo;unsupervised&amp;rdquo;) is an interesting and flexible way to think about the way these training schemes leverage data.
The zero-shot CLIP matches the performance of 4-shot CLIP, which is a surprising result. What do the authors mean when they make this guess about zero-shot&amp;rsquo;s advantage:</description></item><item><title>Distributed representations of words and phrases and their compositionality</title><link>https://kylrth.com/paper/distributed-representations/</link><pubDate>Tue, 01 Feb 2022 16:09:19 -0500</pubDate><guid>https://kylrth.com/paper/distributed-representations/</guid><description>This post was created as an assignment in Bang Liu&amp;rsquo;s IFT6289 course in winter 2022. The structure of the post follows the structure of the assignment: summarization followed by my own comments.
paper summarization permalink This paper describes multiple improvements that are made to the original Skip-gram model:
Decreasing the rate of exposure to common words improves the training speed and increases the model&amp;rsquo;s accuracy on infrequent words. A new training target they call &amp;ldquo;negative sampling&amp;rdquo; improves the training speed and the model&amp;rsquo;s accuracy on frequent words.</description></item><item><title>Cross-lingual alignment of contextual word embeddings, with applications to zero-shot dependency parsing</title><link>https://kylrth.com/paper/cross-lingual-alignment-contextual/</link><pubDate>Fri, 11 Dec 2020 06:30:43 -0700</pubDate><guid>https://kylrth.com/paper/cross-lingual-alignment-contextual/</guid><description>Recent contextual word embeddings (e.g. ELMo) have shown to be much better than &amp;ldquo;static&amp;rdquo; embeddings (where there&amp;rsquo;s a one-to-one mapping from token to representation). This paper is exciting because they were able to create a multi-lingual embedding space that used contextual word embeddings.
Each token will have a &amp;ldquo;point cloud&amp;rdquo; of embedding values, one point for each context containing the token. They define the embedding anchor as the average of all those points for a particular token.</description></item><item><title>SpanBERT: improving pre-training by representing and predicting spans</title><link>https://kylrth.com/paper/spanbert/</link><pubDate>Sat, 05 Dec 2020 16:08:03 -0700</pubDate><guid>https://kylrth.com/paper/spanbert/</guid><description>BERT optimizes the Masked Language Model (MLM) objective by masking word pieces uniformly at random in its training data and attempting to predict the masked values. With SpanBERT, spans of tokens are masked and the model is expected to predict the text in the spans from the representations of the words on the boundary. Span lengths follow a geometric distribution, and span start points are uniformly random.
To predict each individual masked token, a two-layer feedforward network was provided with the boundary token representations plus the position embedding of the target token, and the output vector representation was used to predict the masked token and compute cross-entropy loss exactly as in standard MLM.</description></item><item><title>Deep contextualized word representations</title><link>https://kylrth.com/paper/deep-contextualized-word-representations/</link><pubDate>Thu, 03 Dec 2020 12:01:43 -0700</pubDate><guid>https://kylrth.com/paper/deep-contextualized-word-representations/</guid><description>This is the original paper introducing Embeddings from Language Models (ELMo).
Unlike most widely used word embeddings, ELMo word representations are functions of the entire input sentence.
That&amp;rsquo;s what makes ELMo great: they&amp;rsquo;re contextualized word representations, meaning that they can express multiple possible senses of the same word.
Specifically, ELMo representations are a learned linear combination of all layers of an LSTM encoding. The LSTM undergoes general semi-supervised pretraining, but the linear combination is learned specific to the task.</description></item><item><title>Attention is all you need</title><link>https://kylrth.com/paper/attention-all-you-need/</link><pubDate>Wed, 05 Aug 2020 12:37:42 -0700</pubDate><guid>https://kylrth.com/paper/attention-all-you-need/</guid><description>I also referred to this implementation to understand some of the details.
This is the paper describing the Transformer, a sequence-to-sequence model based entirely on attention. I think it&amp;rsquo;s best described with pictures.
model overview permalink From this picture, I think the following things need explaining:
embeddings these are learned embeddings that convert the input and output tokens to vectors of the model dimension. In this paper, they actually used the same weight matrix for input embedding, output embedding, and the final linear layer before the final softmax.</description></item><item><title>BERT: pre-training of deep bidirectional transformers for language understanding</title><link>https://kylrth.com/paper/bert/</link><pubDate>Tue, 04 Aug 2020 08:57:44 -0700</pubDate><guid>https://kylrth.com/paper/bert/</guid><description>The B is for bidirectional, and that&amp;rsquo;s a big deal. It makes it possible to do well on sentence-level (NLI, question answering) and token-level tasks (NER, POS tagging). In a unidirectional model, the word &amp;ldquo;bank&amp;rdquo; in a sentence like &amp;ldquo;I made a bank deposit.&amp;rdquo; has only &amp;ldquo;I made a&amp;rdquo; as its context, keeping useful information from the model.
Another cool thing is masked language model training (MLM). They train the model by blanking certain words in the sentence and asking the model to guess the missing word.</description></item><item><title>Google's neural machine translation system: bridging the gap between human and machine translation</title><link>https://kylrth.com/paper/google-nmt-2016/</link><pubDate>Tue, 30 Jun 2020 08:22:30 -0600</pubDate><guid>https://kylrth.com/paper/google-nmt-2016/</guid><description>This model was superseded by this one.
They did some careful things with residual connections to make sure it was very parallelizable. They put each LSTM layer on a separate GPU. They quantized the models such that they could train using full floating-point computations with a couple restrictions and then convert the models to quantized versions.</description></item><item><title>Google's multilingual neural machine translation system</title><link>https://kylrth.com/paper/google-zero-shot/</link><pubDate>Fri, 26 Jun 2020 08:02:12 -0600</pubDate><guid>https://kylrth.com/paper/google-zero-shot/</guid><description>They use the word-piece model from &amp;ldquo;Japanese and Korean Voice Search&amp;rdquo;, with 32,000 word pieces. (This is a lot less than the 200,000 used in that paper.) They state in the paper that the shared word-piece model is very similar to Byte-Pair-Encoding, which was used for NMT in this paper by researchers at U of Edinburgh.
The model and training process are exactly as in Google&amp;rsquo;s earlier paper. It takes 3 weeks on 100 GPUs to train, even after increasing batch size and learning rate.</description></item><item><title>Japanese and Korean voice search</title><link>https://kylrth.com/paper/word-piece-model/</link><pubDate>Wed, 24 Jun 2020 14:44:02 -0600</pubDate><guid>https://kylrth.com/paper/word-piece-model/</guid><description>This was mentioned in the paper on Google&amp;rsquo;s Multilingual Neural Machine Translation System. It&amp;rsquo;s regarded as the original paper to use the word-piece model, which is the focus of my notes here.
the WordPieceModel permalink Here&amp;rsquo;s the WordPieceModel algorithm:
func WordPieceModel(D, chars, n, threshold) -&amp;gt; inventory: # D: training data # n: user-specified number of word units (often 200k) # chars: unicode characters used in the language (e.g. Kanji, Hiragana, Katakana, ASCII for Japanese) # threshold: stopping criterion for likelihood increase # inventory: the set of word units created by the model inventory := chars likelihood := +INF while len(inventory) &amp;lt; n &amp;amp;&amp;amp; likelihood &amp;gt;= threshold: lm := LM(inventory, D) inventory += argmax_{combined word unit}(lm.</description></item><item><title>Towards a multi-view language representation</title><link>https://kylrth.com/paper/multi-view-language-representation/</link><pubDate>Tue, 23 Jun 2020 08:40:04 -0600</pubDate><guid>https://kylrth.com/paper/multi-view-language-representation/</guid><description>They used a technique called CCA to combine hand-made features with NN representations. It didn&amp;rsquo;t do great on typological feature prediction, but it did do well with predicting a phylogenetic tree for Indo-European languages.</description></item></channel></rss>