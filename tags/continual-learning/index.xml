<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>continual-learning on Kyle Roth</title><link>https://kylrth.com/tags/continual-learning/</link><description>Recent content in continual-learning on Kyle Roth</description><generator>Hugo -- gohugo.io</generator><language>en-us</language><lastBuildDate>Thu, 02 Jun 2022 15:28:55 -0400</lastBuildDate><atom:link href="https://kylrth.com/tags/continual-learning/index.xml" rel="self" type="application/rss+xml"/><item><title>Continual-T0: progressively instructing 50+ tasks to language models without forgetting</title><link>https://kylrth.com/paper/continual-t0/</link><pubDate>Thu, 02 Jun 2022 15:28:55 -0400</pubDate><guid>https://kylrth.com/paper/continual-t0/</guid><description>This was a paper I presented about in Bang Liu&amp;rsquo;s research group meeting on 2022-06-06. You can view the slides I used here.
Continual-T0 (CT0) extends T0 by progressively training it on 8 unseen language generation tasks, while retaining a replay buffer of 1% of the original training data to preserve performance. The result is a model that maintains nearly all of its performance on previous tasks while learning the new tasks.</description></item></channel></rss>