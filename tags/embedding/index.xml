<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Embedding on Kyle Roth</title><link>https://kylrth.com/tags/embedding/</link><description>Recent content in Embedding on Kyle Roth</description><generator>Hugo</generator><language>en-us</language><lastBuildDate>Tue, 13 Aug 2024 12:06:26 -0400</lastBuildDate><atom:link href="https://kylrth.com/tags/embedding/index.xml" rel="self" type="application/rss+xml"/><item><title>Cross-lingual alignment of contextual word embeddings, with applications to zero-shot dependency parsing</title><link>https://kylrth.com/paper/cross-lingual-alignment-contextual/</link><pubDate>Fri, 11 Dec 2020 06:30:43 -0700</pubDate><guid>https://kylrth.com/paper/cross-lingual-alignment-contextual/</guid><description>Recent contextual word embeddings (e.g. ELMo) have shown to be much better than &amp;ldquo;static&amp;rdquo; embeddings (where there&amp;rsquo;s a one-to-one mapping from token to representation). This paper is exciting because they were able to create a multi-lingual embedding space that used contextual word embeddings.
Each token will have a &amp;ldquo;point cloud&amp;rdquo; of embedding values, one point for each context containing the token. They define the embedding anchor as the average of all those points for a particular token.</description></item><item><title>Deep contextualized word representations</title><link>https://kylrth.com/paper/deep-contextualized-word-representations/</link><pubDate>Thu, 03 Dec 2020 12:01:43 -0700</pubDate><guid>https://kylrth.com/paper/deep-contextualized-word-representations/</guid><description>This is the original paper introducing Embeddings from Language Models (ELMo).
Unlike most widely used word embeddings, ELMo word representations are functions of the entire input sentence.
That&amp;rsquo;s what makes ELMo great: they&amp;rsquo;re contextualized word representations, meaning that they can express multiple possible senses of the same word.
Specifically, ELMo representations are a learned linear combination of all layers of an LSTM encoding. The LSTM undergoes general semi-supervised pretraining, but the linear combination is learned specific to the task.</description></item></channel></rss>