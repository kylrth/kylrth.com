<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>prompting on Kyle Roth</title><link>https://kylrth.com/tags/prompting/</link><description>Recent content in prompting on Kyle Roth</description><generator>Hugo -- gohugo.io</generator><language>en-us</language><lastBuildDate>Tue, 13 Sep 2022 11:26:21 -0400</lastBuildDate><atom:link href="https://kylrth.com/tags/prompting/index.xml" rel="self" type="application/rss+xml"/><item><title>Selective annotation makes language models better few-shot learners</title><link>https://kylrth.com/paper/selective-annotation/</link><pubDate>Tue, 13 Sep 2022 11:26:21 -0400</pubDate><guid>https://kylrth.com/paper/selective-annotation/</guid><description>Selective annotation chooses a pool of samples to annotate from a large set of unlabeled data. The main result of the paper is that when this is combined with item-specific prompt retrieval the performance drastically improves (&amp;gt;10% relative gain and lower performance variance). Interestingly, selective annotation does not help for finetuning, or when the prompts are randomly selected. They call their selective annotation method &amp;ldquo;vote-\(k\)&amp;rdquo;.
selective annotation method permalink Vote-\(k\) essentially creates a network of similaraccording to Sentence-BERT unlabeled instances, and then selects from them with a network importance score that is discounted to promote diversityThe discounting is performed by iteratively adding to the selection set, each time penalizing new nodes for being close to nodes that are already in the selection set.</description></item></channel></rss>